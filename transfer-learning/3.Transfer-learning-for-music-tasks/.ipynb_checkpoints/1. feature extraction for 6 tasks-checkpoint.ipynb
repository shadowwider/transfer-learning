{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `predict()`: save npy files for convnet feature\n",
    "* `get_mfcc()`: save npy file for mfcc (+d, dd) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "import models\n",
    "from argparse import Namespace\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "from keras import backend as K\n",
    "from utils_featext import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A base path for datasets\n",
    "PATH_DATASETS = '/misc/kcgscratch1/ChoGroup/keunwoo/datasets/'\n",
    "# Jamendo is here after trimming\n",
    "PATH_PROCESSED = '/misc/kcgscratch1/ChoGroup/keunwoo/datasets_processed/'\n",
    "PATH_URBANSOUND = '/misc/kcgscratch1/ChoGroup/keunwoo/UrbanSound8K/audio'\n",
    "# A folder to store csv files\n",
    "FOLDER_CSV = 'data_csv/'\n",
    "# A folder to store extracted features\n",
    "FOLDER_FEATS = 'data_feats/'\n",
    "if not os.path.exists(FOLDER_CSV):\n",
    "    os.mkdir(FOLDER_CSV)\n",
    "if not os.path.exists(FOLDER_FEATS):\n",
    "    os.mkdir(FOLDER_FEATS)\n",
    "\n",
    "# Some constants for my convnet\n",
    "SR = 12000 # [Hz]\n",
    "len_src = 29. # [second]\n",
    "N_JOBS = 9\n",
    "ref_n_src = 12000 * 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `PATH_DATASETS`:\n",
    "\n",
    "```shell\n",
    "keunwoo@weaver4[datasets]$ pwd\n",
    "/misc/kcgscratch1/ChoGroup/keunwoo/datasets\n",
    "keunwoo@weaver4[datasets]$ ls -l\n",
    "drwx------. 16 keunwoo keunwoo       4096 Dec 29 16:42 ballroom_extended_2016\n",
    "drwx------.  3 keunwoo keunwoo       4096 Dec 29 16:19 emoMusic45s\n",
    "drwx------.  3 keunwoo keunwoo         27 Dec 29 16:19 gtzan_genre\n",
    "drwx------.  3 keunwoo keunwoo         33 Dec 29 16:20 gtzan_music_speech\n",
    "drwx------.  6 keunwoo keunwoo       4096 Dec 29 18:21 jamendo_voice_activity\n",
    "drwx------.  4 keunwoo keunwoo       4096 Nov 29 02:46 UrbanSound8K\n",
    "```\n",
    "\n",
    "In `PATH_PROCESSED`:\n",
    "\n",
    "```shell\n",
    "keunwoo@weaver4[datasets_processed]$ pwd\n",
    "/misc/kcgscratch1/ChoGroup/keunwoo/datasets_processed\n",
    "keunwoo@weaver4[datasets_processed]$ ls -l\n",
    "drwx------. 5 keunwoo keunwoo 57 Dec 29 19:20 jamendo_trimmed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_filepaths(df, dataroot=None):\n",
    "    \"\"\"Generate file path (column name 'filepath') from given dataframe \"\"\"\n",
    "    if dataroot is None:\n",
    "        dataroot = PATH_DATASETS\n",
    "    for filepath in df['filepath']:\n",
    "        yield os.path.join(dataroot, filepath)\n",
    "\n",
    "def gen_audiofiles(df, batch_size=256, dataroot=None):\n",
    "    '''gen single audio file src in a batch_size=1 form for keras model.predict_generator\n",
    "    df: dataframe \n",
    "    total_size: integer.\n",
    "    batch_size: integer.\n",
    "    dataroot: root path for data'''\n",
    "\n",
    "    ''''''\n",
    "    pool = Pool(N_JOBS)\n",
    "    def _multi_loading(pool, paths):\n",
    "        srcs = pool.map(_load_audio, paths)\n",
    "        srcs = np.array(srcs)\n",
    "        try:\n",
    "            srcs = srcs[:, np.newaxis, :]\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "\n",
    "        return srcs\n",
    "    \n",
    "    total_size = len(df)\n",
    "    n_leftover = int(total_size % batch_size)\n",
    "    leftover = n_leftover != 0\n",
    "    n_batch = int(total_size / batch_size)\n",
    "    gen_f = gen_filepaths(df, dataroot=dataroot)\n",
    "    print('n_batch: {}, n_leftover: {}, all: {}'.format(n_batch, n_leftover, total_size))\n",
    "    \n",
    "    for batch_idx in xrange(n_batch):\n",
    "        paths = []\n",
    "        for inbatch_idx in range(batch_size):\n",
    "            paths.append(gen_f.next())\n",
    "        print('..yielding {}/{} batch..'.format(batch_idx, n_batch))                    \n",
    "        yield _multi_loading(pool, paths)\n",
    "        \n",
    "    if leftover:\n",
    "        paths = []\n",
    "        for inbatch_idx in range(n_leftover):\n",
    "            paths.append(gen_f.next())\n",
    "        print('..yielding final batch w {} data sample..'.format(len(paths)))\n",
    "        yield _multi_loading(pool, paths)\n",
    "\n",
    "def _load_audio(path):\n",
    "    \"\"\"Load audio file at path with sampling rate=SR, duration=len_src, and return it\"\"\"\n",
    "    src, sr = librosa.load(path, sr=SR, duration=len_src * SR / float(SR))\n",
    "    src = src[:ref_n_src]\n",
    "    result = np.zeros(ref_n_src)\n",
    "    result[:len(src)] = src[:ref_n_src]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for convnet feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(filename, batch_size, model, dataroot=None, npy_suffix=''):\n",
    "    \"\"\"Extract convnet feature using given model\"\"\"\n",
    "    if dataroot is None:\n",
    "        dataroot = PATH_DATASETS\n",
    "    start = time.time()\n",
    "    csv_filename = '{}.csv'.format(filename)\n",
    "    npy_filename = '{}{}.npy'.format(filename, npy_suffix)\n",
    "    df = pd.DataFrame.from_csv(os.path.join(FOLDER_CSV, csv_filename))\n",
    "    print('{}: Dataframe with size:{}').format(filename, len(df))\n",
    "    example_path = os.path.join(dataroot, df['filepath'][0])\n",
    "    print('An example path - does it exists? {}'.format(os.path.exists(example_path)))\n",
    "    print(df.columns)\n",
    "    gen_audio = gen_audiofiles(df, batch_size, dataroot)\n",
    "    feats = model.predict_generator(generator=gen_audio, \n",
    "                                    val_samples=len(df), \n",
    "                                    max_q_size=1)\n",
    "    np.save(os.path.join(FOLDER_FEATS, npy_filename), feats)\n",
    "    print('DONE! in {:6.4f} sec'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for mfcc\n",
    "def get_mfcc(filename, dataroot=None):    \n",
    "    start = time.time()\n",
    "    csv_filename = '{}.csv'.format(filename)\n",
    "    npy_filename = '{}_mfcc.npy'.format(filename)\n",
    "    df = pd.DataFrame.from_csv(os.path.join(FOLDER_CSV, csv_filename))\n",
    "    print('{}: Dataframe with size:{}').format(filename, len(df))\n",
    "    print(os.path.exists(os.path.join(dataroot, df['filepath'][0])))   \n",
    "    print(df.columns)\n",
    "    gen_f = gen_filepaths(df, dataroot=dataroot)\n",
    "\n",
    "    pool = Pool(N_JOBS)\n",
    "    paths = list(gen_f)\n",
    "    feats = pool.map(_path_to_mfccs, paths)\n",
    "    feats = np.array(feats)\n",
    "    np.save(os.path.join(FOLDER_FEATS, npy_filename), feats)\n",
    "    print('MFCC is done! in {:6.4f} sec'.format(time.time() - start))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "def _path_to_mfccs(path):\n",
    "    src_zeros = np.zeros(1024) # min length to have 3-frame mfcc's\n",
    "    src, sr = librosa.load(path, sr=SR, duration=29.) # max len: 29s, can be shorter.\n",
    "    if len(src) < 1024:\n",
    "        src_zeros[:len(src)] = src\n",
    "        src = src_zeros\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(src, SR, n_mfcc=20)\n",
    "    dmfcc = mfcc[:, 1:] - mfcc[:, :-1]\n",
    "    ddmfcc = dmfcc[:, 1:] - dmfcc[:, :-1]\n",
    "    return np.concatenate((np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
    "                           np.mean(dmfcc, axis=1), np.std(dmfcc, axis=1),\n",
    "                           np.mean(ddmfcc, axis=1), np.std(ddmfcc, axis=1))\n",
    "                          , axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract my convnet features\n",
    "## Models for layer 1-5 (or, 0-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "tasks.append(['ballroom_extended', 'gtzan_speechmusic'])\n",
    "tasks.append(['emoMusic', 'jamendo_vd', 'urbansound', 'gtzan_genre', ])\n",
    "\n",
    "dataroots = []\n",
    "dataroots.append([None, None])\n",
    "dataroots.append([None,\n",
    "                  PATH_PROCESSED,\n",
    "                  PATH_URBANSOUND,\n",
    "                  None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gnu/anaconda/lib/python2.7/site-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", input_shape=(96, 1360,...)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The `mode` argument of `BatchNormalization` no longer exists. `mode=1` and `mode=2` are no longer supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7f3f0db05133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmid_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_for_mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnpy_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_layer_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataroots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nStart: mid_idx: {}, task_idx: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gnu/Gnubox/transfer_learning_music_platform/utils_featext.pyc\u001b[0m in \u001b[0;36mload_model_for_mid\u001b[0;34m(mid_idx)\u001b[0m\n\u001b[1;32m    110\u001b[0m                      \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_fb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_kernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                      conv_until=mid_idx)\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_convnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     model.load_weights(os.path.join(FOLDER_WEIGHTS, 'weights_layer{}_{}.hdf5'.format(mid_idx, K._backend)),\n\u001b[1;32m    114\u001b[0m                        by_name=True)\n",
      "\u001b[0;32m/Users/gnu/Gnubox/transfer_learning_music_platform/models_transfer.py\u001b[0m in \u001b[0;36mbuild_convnet_model\u001b[0;34m(args, last_layer, sr)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdecibel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecibel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     model = raw_vgg(args, tf=tf, normalize=normalize, decibel=decibel,\n\u001b[0;32m---> 36\u001b[0;31m                     last_layer=last_layer, sr=sr)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ConvBNEluDr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gnu/Gnubox/transfer_learning_music_platform/models_transfer.py\u001b[0m in \u001b[0;36mraw_vgg\u001b[0;34m(args, input_length, tf, normalize, decibel, last_layer, sr)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mpoolings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             0.0, model.output_shape[1:]]\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_convBNeluMPdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nin_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_until\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_until\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconv_until\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gnu/Gnubox/transfer_learning_music_platform/models_transfer.py\u001b[0m in \u001b[0;36mget_convBNeluMPdrop\u001b[0;34m(num_conv_layers, nums_feat_maps, feat_scale_factor, conv_sizes, pool_sizes, dropout_conv, input_shape, num_nin_layers, conv_until)\u001b[0m\n\u001b[1;32m    168\u001b[0m                                         init='he_normal'))\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# add BN, Activation, pooling, and dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvanced_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mELU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: select activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gnu/anaconda/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mobject_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gnu/anaconda/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mbatchnorm_args_preprocessor\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             raise TypeError('The `mode` argument of `BatchNormalization` '\n\u001b[0m\u001b[1;32m    437\u001b[0m                             \u001b[0;34m'no longer exists. `mode=1` and `mode=2` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             'are no longer supported.')\n",
      "\u001b[0;31mTypeError\u001b[0m: The `mode` argument of `BatchNormalization` no longer exists. `mode=1` and `mode=2` are no longer supported."
     ]
    }
   ],
   "source": [
    "for mid_idx in range(5):\n",
    "    model = load_model_for_mid(mid_idx)\n",
    "    npy_suffix = '_layer_{}'.format(mid_idx)\n",
    "    for task_idx, (filename, dr) in enumerate(zip(tasks, dataroots)):\n",
    "        print('\\nStart: mid_idx: {}, task_idx: {}'.format(mid_idx, task_idx))\n",
    "        predict(filename, batch_size, model, dr, npy_suffix)\n",
    "        print('Done: mid_idx: {}, task_idx: {}'.format(mid_idx, task_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Due to the Keras 2.0 API change (BatchNormalization) my model can't be loaded. You can use Keras 1.1 to reproduce it, or just use features under `data_feats` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO THE JOBS Set 1 - task 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in ['ballroom_extended', 'gtzan_genre', 'gtzan_speechmusic']:\n",
    "    get_mfcc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tasks = ['emoMusic', 'jamendo_vd', 'urbansound']\n",
    "dataroots = [None, \n",
    "             PATH_PROCESSED,\n",
    "             os.path.join(PATH_URBANSOUND, 'audio')]\n",
    "for idx, (filename, dr) in enumerate(zip(tasks, dataroots)):\n",
    "    get_mfcc(filename, dataroot=dr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
